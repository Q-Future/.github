# Low-level Visual Perception in the Foundation Model Era
⭐ _keywords_: Low-level Visual Perception | Multi-Modality Large Language Models | Visual Quality Assessment

#### What have we published?

- **Q-Bench**: [Homepage](https://q-future.github.io/Q-Bench/), [Repo](https://github.com/Q-Future/Q-Bench), [Data](https://github.com/Q-Future/Q-Bench/releases/tag/v1.0.1.1014datarelease), [Preprint](https://arxiv.org/abs/2309.14181) The first low-level benchmark for foundation models on low-level vision.

#### What are we working on?

- **[WIP]Chinese-Q-Bench/质衡**: [Homepage](https://q-future.github.io/Chinese-Q-Bench/), [Repo](https://github.com/Q-Future/Chinese-Q-Bench) The first attempt to test multi-lingual abilities on low-level vision.
- **[WIP]Q-Instruct**: [Homepage (TBA)](NA), [Repo (TBA)](NA), [Data](https://huggingface.co/datasets/nanyangtu/Q-Instruct) A large-scale instruction tuning dataset to improve low-level perceptual abilities of foundation models.


Maintained by [Teo Wu](https://github.com/teowu)@Singapore and [Zicheng Zhang](https://github.com/zzc-1998)@Shanghai.
